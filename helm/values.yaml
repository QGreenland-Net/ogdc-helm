# Default values for OGDC
## This is a YAML-formatted file.
##
##     * * *   NOTE: SOME VALUES INCLUDE THE PLACEHOLDER "${RELEASE_NAME}",   * * *
##     * * *         WHICH MUST BE REPLACED BY THE ACTUAL RELEASE NAME        * * *
##
## Make sure you're in the correct k8s context, then:
##  $  helm upgrade --install ns -n ogdc -f values-dev-cluster-ogdc.yaml \
##         oci://ghcr.io/qgreenland-net/charts/ogdc --version [version] \
##             --debug --render-subchart-notes
##


## @section Global parameters
## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
##v

## @section Global Configuration
## Global configuration parameters for OGDC
##
global:
  ## @param global.passwordsSecret The name of the Secret containing application passwords
  ## This is the secret deployed using your own private, edited version of `../admin/secrets.yaml`.
  ##
  ## (Note the actual key passwordsSecret is not used anywhere; only the yaml anchor/alias
  ##  &passwordSecretName. IF YOU OVERRIDE THIS VALUE IN ANOTHER VALUES-OVERLAY FILE, YOU ALSO
  ## NEED TO OVERRIDE THE VALUES THAT USE THIS ANCHOR - search this file for 'passwordSecretName')
  ##
  ## IMPORTANT NOTE: make sure you edit this, since IT INCLUDES THE RELEASE NAME! For
  ## example, if the release name is 'myrelease', the value of name: would be
  ## 'myrelease-secrets'.
  ##
  passwordsSecret: &passwordSecretName "myrelease-secrets"

  ## @param global.defaultStorageClass Global default StorageClass for Persistent Volume(s)
  ##
  ## Inspect your cluster to see what storageClass is set as default:
  ##    $  kubectl get storageclass
  ## ...and then explicitly set defaultStorageClass to match the name of the default storageclass
  ## (e.g. for Rancher Desktop, use:   defaultStorageClass: local-path)
  ##
  defaultStorageClass: local-path

## @section Argo Workflows Configuration
## argo-workflows configuration
## See https://argoproj.github.io/argo-workflows/
##
argo-workflows:
  ## @param argo-workflows.singleNamespace Restrict Argo to operate only in a single namespace
  ## Restrict Argo to operate only in a single namespace (the namespace of the
  ## Helm release) by apply Roles and RoleBindings instead of the Cluster
  ## equivalents, and start workflow-controller with the --namespaced flag. Use it
  ## in clusters with strict access policy.
  ##
  singleNamespace: false

  ## Custom resource configuration
  crds:
    ## @param argo-workflows.crds.install Install and upgrade CRDs
    install: true

    ## @param argo-workflows.crds.keep Keep CRDs on chart uninstall
    keep: false

    ## @param argo-workflows.crds.annotations Annotations to be added to all CRDs
    annotations: {}

  ## @param argo-workflows.createAggregateRoles Create ClusterRoles that extend existing ClusterRoles to interact with Argo Workflows CRDs
  ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles
  ##
  createAggregateRoles: true

  ## Workflow service account configuration
  workflow:
    ## @param argo-workflows.workflow.serviceAccount.create Specifies whether a service account should be created
    ## @param argo-workflows.workflow.serviceAccount.name Service account which is used to run workflows
    ## @param argo-workflows.workflow.serviceAccount.automountServiceAccountToken Automount service account token for the workflow pods
    serviceAccount:
      create: true
      name: "argo-workflow"
      automountServiceAccountToken: true

    ## @param argo-workflows.workflow.rbac.create Adds Role and RoleBinding for the above specified service account
    ## Adds Role and RoleBinding for the above specified service account to be able to run workflows.
    ## A Role and Rolebinding pair is also created for each namespace in controller.workflowNamespaces (see below)
    ##
    rbac:
      create: true

  ## Controller configuration
  controller:
    ## @param argo-workflows.controller.resources.requests.memory Memory requests for the workflow controller
    ## @param argo-workflows.controller.resources.requests.cpu CPU requests for the workflow controller
    ## @param argo-workflows.controller.resources.limits.memory Memory limits for the workflow controller
    ## @param argo-workflows.controller.resources.limits.cpu CPU limits for the workflow controller
    resources:
      requests:
        memory: "2Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "1000m"

    ## @param argo-workflows.controller.workflowNamespaces Specify all namespaces where this workflow controller instance will manage workflows
    ## Specify all namespaces where this workflow controller instance will manage
    ## workflows. This controls where the service account and RBAC resources will
    ## be created. Only valid when singleNamespace is false.
    ##
    workflowNamespaces:
      - qgnet

    ## @param argo-workflows.controller.workflowDefaults.spec.podGC.strategy Automatically cleanup pods on successful workflow completion
    ## Ref: https://argo-workflows.readthedocs.io/en/latest/fields/#podgc
    ##
    workflowDefaults:
      spec:
        podGC:
          strategy: OnWorkflowSuccess
          
    ## @param argo-workflows.controller.metricsConfig.enabled Enables prometheus metrics server
    ## Configure metrics
    ## Ref: https://argoproj.github.io/argo-workflows/metrics/
    ##
    metricsConfig:
      enabled: false

  ## Argo Server configuration
  server:
    ## @param argo-workflows.server.enabled Deploy the Argo Server
    enabled: true

    ## @param argo-workflows.server.resources.requests.memory Memory requests for the workflow server
    ## @param argo-workflows.server.resources.requests.cpu CPU requests for the workflow server
    ## @param argo-workflows.server.resources.limits.memory Memory limits for the workflow server
    ## @param argo-workflows.server.resources.limits.cpu CPU limits for the workflow server
    ## Set resource requirements
    ## Ref: https://argoproj.github.io/argo-workflows/argo-server/
    ##
    resources:
      requests:
        memory: "1Gi"
        cpu: "200m"
      limits:
        memory: "2Gi"
        cpu: "500m"

    ## @param argo-workflows.server.authModes A list of supported authentication modes
    ## Available values are `server`, `client`, or `sso`. If you provide sso, please configure `.Values.server.sso` as well.
    ## Ref: https://argo-workflows.readthedocs.io/en/stable/argo-server-auth-mode/
    ##
    authModes:
      - server

    ## Ingress configuration.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ##
    ingress:
      ## @param argo-workflows.server.ingress.enabled Enable an ingress resource for the Argo server
      enabled: false

      ## @param argo-workflows.server.ingress.ingressClassName Ingress class name
      ## Your cluster may require this to be set to match your IngressClass resource.
      ## For nginx ingress controller, use:  ingressClassName: "nginx"
      ##
      ingressClassName: "nginx"

      ## @param argo-workflows.server.ingress.annotations [object] Additional ingress annotations
      ## Add any additional annotations that your ingress controller may need.
      ## For example, for cert-manager TLS using the 'letsencrypt-prod' ClusterIssuer
      ## (which must be created separately), and to specify that the backend protocol
      ## is HTTPS, use the following annotations:
      ##
      annotations: { 
        cert-manager.io/cluster-issuer: letsencrypt-prod,
        nginx.ingress.kubernetes.io/rewrite-target: /$2,
        nginx.ingress.kubernetes.io/use-regex: "true"
      }

      ## @param argo-workflows.server.ingress.hosts Ingress hosts
      ## Add host names (without http(s):// or path) that map to the argo server service.
      ## You may need to add multiple hosts if you have multiple DNS names that map to the
      ## same service.
      ##
      hosts:
        - api.test.dataone.org

      ## @param argo-workflows.server.ingress.paths Ingress endpoint paths
      ## If you are using a path other than '/', make sure to also set
      ## server.baseHref to match!
      ##
      paths:
        - /ogdc(/|$)(.*)

      ## @param argo-workflows.server.ingress.pathType Path type
      ## Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types
      ##
      pathType: ImplementationSpecific

      ## @param argo-workflows.server.ingress.servicePort Ingress service backend port
      ## This is the port for the argo server service.
      ##
      servicePort: 2746

      ## @param argo-workflows.server.ingress.tls [array] Ingress TLS configuration
      ## Add TLS configuration for the Ingress, if desired.
      ## This example assumes you have created a TLS secret named
      ## 'ingress-nginx-tls-cert' in the same namespace as the argo server.
      ## The TLS secret must contain the certificate and private key for the host(s)
      ## specified above.
      ##
      tls:
        - hosts:
            - api.test.dataone.org
          secretName: ingress-nginx-tls-cert

  ## Argo Executor configuration
  executor:
    ## @param argo-workflows.executor.resources.requests.memory Memory requests for the workflow executor
    ## @param argo-workflows.executor.resources.requests.cpu CPU requests for the workflow executor
    ## @param argo-workflows.executor.resources.limits.memory Memory limits for the workflow executor
    ## @param argo-workflows.executor.resources.limits.cpu CPU limits for the workflow executor
    ## Set resource requirements
    ## Ref: https://argoproj.github.io/argo-workflows/executor/
    ##
    resources:
      requests:
        memory: "1Gi"
        cpu: "200m"
      limits:
        memory: "4Gi"
        cpu: "1000m"

  ## Artifact repository configuration
  artifactRepository:
    ## @param argo-workflows.artifactRepository.archiveLogs Archive the main container logs as an artifact
    archiveLogs: false

    ## @param argo-workflows.artifactRepository.s3.bucket S3 bucket name
    ## @param argo-workflows.artifactRepository.s3.endpoint S3 endpoint
    ## @param argo-workflows.artifactRepository.s3.accessKeySecret.name Secret name containing S3 access key
    ## @param argo-workflows.artifactRepository.s3.accessKeySecret.key Secret key for S3 access key
    ## @param argo-workflows.artifactRepository.s3.secretKeySecret.name Secret name containing S3 secret key
    ## @param argo-workflows.artifactRepository.s3.secretKeySecret.key Secret key for S3 secret key
    ## @param argo-workflows.artifactRepository.s3.insecure Disable TLS for S3 connection
    ## Store artifact in a S3-compliant object store
    ##
    s3:
      bucket: argo-workflows
      endpoint: "{{ .Release.Name }}-minio:9000"

      ## Note the `key` attribute is not the actual secret, it's the PATH to
      ## the contents in the associated secret, as defined by the `name` attribute.
      accessKeySecret:
        name: *passwordSecretName
        key: rootUser
      secretKeySecret:
        name: *passwordSecretName
        key: rootPassword

      ## insecure will disable TLS. Primarily used for minio installs not configured with TLS
      insecure: true

## @section MinIO Configuration
## MinIO configuration
## See https://min.io/docs/minio/kubernetes/upstream/
##
minio:
  ## @param minio.mode MinIO mode (standalone or distributed)
  mode: standalone ## other supported values are "distributed"

  ## @param minio.replicas Number of MinIO containers running
  replicas: 1

  ## @param minio.buckets [array] List of buckets to be created after minio install
  buckets:
    - name: argo-workflows
      ## Policy to be set on the bucket [none|download|upload|public]
      policy: none
      ## Purge if bucket exists already
      purge: false

  ## @param minio.persistence.enabled Enable persistence using Persistent Volume Claims
  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    enabled: true

  ## @param minio.existingSecret Use existing Secret that store MinIO credentials
  ## Use existing Secret that store following variables:
  ##
  ## | Chart var             | .data.<key> in Secret    |
  ## |:----------------------|:-------------------------|
  ## | rootUser              | rootUser                 |
  ## | rootPassword          | rootPassword             |
  ##
  ## All mentioned variables will be ignored in values file.
  ## .data.rootUser and .data.rootPassword are mandatory,
  ## others depend on enabled status of corresponding sections.
  ##
  existingSecret: *passwordSecretName

  ## @param minio.resources.requests.memory Memory requests for minio
  ## @param minio.resources.requests.cpu CPU requests for minio
  ## @param minio.resources.limits.memory Memory limits for minio
  ## @param minio.resources.limits.cpu CPU limits for minio
  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    requests:
      memory: "1Gi"
      cpu: "200m"
    limits:
      memory: "2Gi"
      cpu: "500m"

## @section OGDC Configuration
## OGDC application configuration
##

## @param image.repository OGDC container image repository
## @param image.tag OGDC container image tag
image:
  repository: ogdc-runner
  tag: latest

## @param ogdc_service_command Command to run the OGDC service
## Default to production fastapi run command
##
ogdc_service_command: ". ./.venv/bin/activate && fastapi run --port 8000 --host 0.0.0.0 src/ogdc_runner/service.py"
